{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLflow Experiment Analysis\n",
    "\n",
    "This notebook helps you analyze your MLflow experiments and visualize model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set MLflow tracking URI\n",
    "mlflow.set_tracking_uri(\"../mlruns\")\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all experiments\n",
    "experiments = mlflow.search_experiments()\n",
    "\n",
    "print(\"Available Experiments:\")\n",
    "for exp in experiments:\n",
    "    print(f\"  - {exp.name} (ID: {exp.experiment_id})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select an experiment (change the name as needed)\n",
    "experiment_name = \"model_validation\"\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "\n",
    "if experiment:\n",
    "    print(f\"✓ Loaded experiment: {experiment_name}\")\n",
    "    print(f\"  Experiment ID: {experiment.experiment_id}\")\n",
    "else:\n",
    "    print(f\"✗ Experiment '{experiment_name}' not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all runs from the experiment\n",
    "if experiment:\n",
    "    runs = mlflow.search_runs(\n",
    "        experiment_ids=[experiment.experiment_id],\n",
    "        order_by=[\"start_time DESC\"]\n",
    "    )\n",
    "    \n",
    "    print(f\"Total runs: {len(runs)}\")\n",
    "    print(f\"\\nColumns: {list(runs.columns)}\")\n",
    "    \n",
    "    # Display first few runs\n",
    "    runs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get metric columns\n",
    "metric_cols = [col for col in runs.columns if col.startswith('metrics.')]\n",
    "\n",
    "if metric_cols:\n",
    "    print(\"Metric Summary:\\n\")\n",
    "    print(runs[metric_cols].describe())\n",
    "else:\n",
    "    print(\"No metrics found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latency distribution\n",
    "if 'metrics.latency' in runs.columns:\n",
    "    fig = px.histogram(\n",
    "        runs,\n",
    "        x='metrics.latency',\n",
    "        nbins=30,\n",
    "        title='Response Latency Distribution',\n",
    "        labels={'metrics.latency': 'Latency (seconds)'},\n",
    "        template='plotly_white'\n",
    "    )\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"No latency data available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality scores over time\n",
    "if 'metrics.quality_score' in runs.columns:\n",
    "    fig = px.scatter(\n",
    "        runs,\n",
    "        x='start_time',\n",
    "        y='metrics.quality_score',\n",
    "        color='params.model',\n",
    "        title='Quality Score Over Time',\n",
    "        labels={'metrics.quality_score': 'Quality Score', 'start_time': 'Time'},\n",
    "        template='plotly_white',\n",
    "        hover_data=['params.temperature']\n",
    "    )\n",
    "    fig.update_traces(marker=dict(size=10))\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"No quality score data available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple metrics comparison\n",
    "metrics_to_plot = ['metrics.coherence_score', 'metrics.relevance_score', 'metrics.safety_score']\n",
    "available_metrics = [m for m in metrics_to_plot if m in runs.columns]\n",
    "\n",
    "if available_metrics:\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    for metric in available_metrics:\n",
    "        metric_name = metric.replace('metrics.', '').replace('_', ' ').title()\n",
    "        fig.add_trace(go.Box(\n",
    "            y=runs[metric],\n",
    "            name=metric_name\n",
    "        ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='Quality Metrics Distribution',\n",
    "        yaxis_title='Score',\n",
    "        template='plotly_white',\n",
    "        showlegend=True\n",
    "    )\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"No quality metrics available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latency vs Quality scatter\n",
    "if 'metrics.latency' in runs.columns and 'metrics.quality_score' in runs.columns:\n",
    "    fig = px.scatter(\n",
    "        runs,\n",
    "        x='metrics.latency',\n",
    "        y='metrics.quality_score',\n",
    "        color='params.temperature',\n",
    "        size='metrics.total_tokens',\n",
    "        title='Latency vs Quality Score',\n",
    "        labels={\n",
    "            'metrics.latency': 'Latency (seconds)',\n",
    "            'metrics.quality_score': 'Quality Score',\n",
    "            'params.temperature': 'Temperature'\n",
    "        },\n",
    "        template='plotly_white',\n",
    "        hover_data=['params.model']\n",
    "    )\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"Insufficient data for latency vs quality plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models if multiple models were tested\n",
    "if 'params.model' in runs.columns:\n",
    "    model_comparison = runs.groupby('params.model').agg({\n",
    "        'metrics.latency': ['mean', 'std'],\n",
    "        'metrics.quality_score': ['mean', 'std'],\n",
    "        'metrics.tokens_per_second': ['mean', 'std']\n",
    "    }).round(3)\n",
    "    \n",
    "    print(\"Model Comparison:\\n\")\n",
    "    print(model_comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model performance radar chart\n",
    "if 'params.model' in runs.columns and len(runs['params.model'].unique()) > 1:\n",
    "    models = runs['params.model'].unique()\n",
    "    \n",
    "    categories = ['Quality', 'Coherence', 'Relevance', 'Speed']\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    for model in models:\n",
    "        model_data = runs[runs['params.model'] == model]\n",
    "        \n",
    "        values = [\n",
    "            model_data['metrics.quality_score'].mean(),\n",
    "            model_data['metrics.coherence_score'].mean(),\n",
    "            model_data['metrics.relevance_score'].mean(),\n",
    "            1 - (model_data['metrics.latency'].mean() / 10)  # Normalized speed score\n",
    "        ]\n",
    "        \n",
    "        fig.add_trace(go.Scatterpolar(\n",
    "            r=values,\n",
    "            theta=categories,\n",
    "            fill='toself',\n",
    "            name=model\n",
    "        ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        polar=dict(\n",
    "            radialaxis=dict(\n",
    "                visible=True,\n",
    "                range=[0, 1]\n",
    "            )\n",
    "        ),\n",
    "        title='Model Performance Comparison',\n",
    "        template='plotly_white'\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Temperature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of temperature on quality\n",
    "if 'params.temperature' in runs.columns and 'metrics.quality_score' in runs.columns:\n",
    "    temp_analysis = runs.groupby('params.temperature').agg({\n",
    "        'metrics.quality_score': ['mean', 'std', 'count'],\n",
    "        'metrics.latency': 'mean'\n",
    "    }).round(3)\n",
    "    \n",
    "    print(\"Temperature Effect:\\n\")\n",
    "    print(temp_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Best Performing Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 runs by quality score\n",
    "if 'metrics.quality_score' in runs.columns:\n",
    "    top_runs = runs.nlargest(10, 'metrics.quality_score')[[\n",
    "        'run_id',\n",
    "        'params.model',\n",
    "        'params.temperature',\n",
    "        'metrics.quality_score',\n",
    "        'metrics.latency',\n",
    "        'metrics.tokens_per_second'\n",
    "    ]]\n",
    "    \n",
    "    print(\"Top 10 Runs by Quality:\\n\")\n",
    "    print(top_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CSV\n",
    "output_file = '../experiment_results.csv'\n",
    "runs.to_csv(output_file, index=False)\n",
    "print(f\"✓ Results exported to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Custom Analysis\n",
    "\n",
    "Add your own analysis here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your custom analysis code here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
