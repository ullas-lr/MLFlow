======================================================================
COMPLETE FDP SESSION SLIDES
Cloud Platforms, Edge AI, Model Validation & Monitoring
Ullas L R - Feb 6, 2026
======================================================================

SLIDE 1: TITLE
Cloud Platforms, Edge AI, Model Validation & Monitoring
Complete MLOps Pipeline from Cloud to Edge

Ullas L R
FDP Session - Feb 2, 2026

==========

SLIDE 2: AGENDA
Part 1: Cloud Platforms (15 min)
‚Ä¢ AWS, Azure, GCP overview
‚Ä¢ ML services comparison
‚Ä¢ Healthcare considerations
‚Ä¢ Discussion

Part 2: Edge AI (15 min)
‚Ä¢ What is Edge AI?
‚Ä¢ Why critical for healthcare
‚Ä¢ Real-world use cases
‚Ä¢ Edge vs Cloud vs Hybrid
‚Ä¢ Discussion

Part 3: Model Validation (25 min)
‚Ä¢ Quality & safety testing
‚Ä¢ Live demo + walkthrough
‚Ä¢ MLflow tracking
‚Ä¢ Deep dive on results
‚Ä¢ Discussion

Part 4: Model Monitoring (25 min)
‚Ä¢ Drift detection concepts
‚Ä¢ Live demo + walkthrough
‚Ä¢ Dashboard visualization
‚Ä¢ Production implications
‚Ä¢ Discussion

Wrap-up (10 min) + Q&A (15 min)
Total: 105 minutes (9:30-11:15 AM)

==========

======== PART 1: CLOUD PLATFORMS ========

SLIDE 3: CLOUD PLATFORMS OVERVIEW
Three Major Providers:

AWS (Amazon Web Services)
‚Ä¢ ML: SageMaker, Bedrock, Comprehend Medical
‚Ä¢ Compute: EC2, Lambda
‚Ä¢ Market leader, most mature

Azure (Microsoft)
‚Ä¢ ML: Azure ML, Cognitive Services, Health Bot
‚Ä¢ Compute: VMs, Container Instances
‚Ä¢ Enterprise focus, Microsoft ecosystem

GCP (Google Cloud)
‚Ä¢ ML: Vertex AI, AutoML, Healthcare API
‚Ä¢ Compute: Compute Engine, Cloud Run
‚Ä¢ AI/ML leader, cutting-edge

All three: HIPAA-compliant with BAA

==========

SLIDE 4: CLOUD ML SERVICES COMPARISON
AWS vs Azure vs GCP:

Feature          | AWS          | Azure       | GCP
ML Platform      | SageMaker    | Azure ML    | Vertex AI
Healthcare API   | Comprehend   | Health Bot  | Healthcare API
GPU Instances    | P4d/P5       | NC/ND       | A2/A3
Pricing          | $$$          | $$$         | $$
Ease of Use      | Medium       | Easy        | Easy

Healthcare Focus:
‚úÖ All HIPAA-compliant
‚úÖ Support PHI workloads
‚úÖ Different strengths

==========

SLIDE 5: CLOUD FOR HEALTHCARE
HIPAA Compliance:
‚úÖ BAA required with cloud provider
‚úÖ Specific configurations needed
‚úÖ Not all services eligible

Data Residency:
‚Ä¢ PHI must stay in specific regions
‚Ä¢ GDPR (EU), HIPAA (US)

Security Features:
‚Ä¢ Encryption (at rest & transit)
‚Ä¢ Network isolation (VPC/VNet)
‚Ä¢ Access controls (IAM)
‚Ä¢ Audit logging

Real-World Examples:
‚Ä¢ AWS: Cleveland Clinic, Moderna
‚Ä¢ Azure: Mayo Clinic, Humana
‚Ä¢ GCP: Stanford Healthcare

==========

SLIDE 5A: CLOUD ML WORKFLOW
Typical Cloud ML Pipeline:

1. Data Collection
   ‚Üí S3/Blob Storage/Cloud Storage

2. Training
   ‚Üí SageMaker/Azure ML/Vertex AI
   ‚Üí GPU instances (P4, NC-series, A2)

3. Model Registry
   ‚Üí SageMaker Registry/Azure ML/Vertex

4. Deployment
   ‚Üí Endpoints/API Gateway
   ‚Üí Auto-scaling

5. Monitoring
   ‚Üí CloudWatch/Monitor/Cloud Logging

Advantages:
‚úÖ Fully managed
‚úÖ Scales automatically
‚úÖ Pay for usage

Challenges:
‚ùå Cost at scale
‚ùå Latency for real-time
‚ùå Data must leave premise

==========

SLIDE 5B: CLOUD COSTS EXAMPLE
Real-World Pricing (Approximate):

AWS SageMaker (ml.p3.2xlarge - 1 GPU):
‚Ä¢ Training: ~$3.06/hour
‚Ä¢ Inference: ~$3.81/hour
‚Ä¢ 24/7 inference: ~$2,750/month
‚Ä¢ Storage (S3): $0.023/GB/month
‚Ä¢ API calls: $0.0004 per request

Azure ML (NC6 - 1 GPU):
‚Ä¢ Training: ~$2.70/hour
‚Ä¢ Inference: ~$3.06/hour
‚Ä¢ 24/7 inference: ~$2,200/month
‚Ä¢ Storage: Similar to AWS

GCP Vertex AI (n1-standard-4 + GPU):
‚Ä¢ Training: ~$2.48/hour
‚Ä¢ Inference: ~$2.80/hour
‚Ä¢ 24/7 inference: ~$2,000/month

100,000 requests/month:
‚Ä¢ Cloud: $2,000-3,000/month
‚Ä¢ Edge: $10,000 hardware (one-time) + $200 power

Breakeven: ~4-6 months for edge!

==========

SLIDE 5C: DISCUSSION - CLOUD PLATFORMS
Questions to engage audience:

‚Ä¢ Who has used AWS/Azure/GCP for ML?
‚Ä¢ What challenges have you faced?
‚Ä¢ Healthcare compliance concerns?
‚Ä¢ Cost experiences?

Key Points:
‚Ä¢ Cloud is powerful but not always the answer
‚Ä¢ Healthcare has unique constraints
‚Ä¢ Edge AI offers alternatives
‚Ä¢ Let's explore edge AI next...

==========

======== PART 2: EDGE AI ========

SLIDE 6: WHAT IS EDGE AI?
Definition:
Edge AI = Running AI models on local devices (not cloud)

Edge Devices:
‚Ä¢ Hospital servers
‚Ä¢ Medical devices
‚Ä¢ IoT devices
‚Ä¢ Mobile phones
‚Ä¢ Edge gateways

Key Characteristics:
‚úÖ Local processing
‚úÖ Low latency (<100ms)
‚úÖ Privacy-first (data stays local)
‚úÖ Works offline
‚úÖ Reduced bandwidth

vs Cloud: Centralized vs Distributed

==========

SLIDE 7: WHY EDGE AI FOR HEALTHCARE?
Critical Advantages:

1. Privacy & Compliance
   ‚Ä¢ PHI never leaves hospital
   ‚Ä¢ HIPAA easier
   ‚Ä¢ Patient trust

2. Low Latency
   ‚Ä¢ Real-time diagnostics (<1 second)
   ‚Ä¢ Critical for emergency care
   ‚Ä¢ Example: Sepsis detection

3. Reliability
   ‚Ä¢ Works without internet
   ‚Ä¢ No cloud outage risk
   ‚Ä¢ Example: Rural clinics

4. Cost
   ‚Ä¢ No cloud API charges
   ‚Ä¢ One-time hardware
   ‚Ä¢ Predictable costs

5. Data Sovereignty
   ‚Ä¢ Compliance with local laws
   ‚Ä¢ GDPR, etc.

==========

SLIDE 8: EDGE AI USE CASES - HEALTHCARE
Real-World Applications:

1. Medical Imaging
   ‚Ä¢ CT/MRI analysis at imaging station
   ‚Ä¢ Brain hemorrhage detection in ER
   ‚Ä¢ Latency: <5 seconds

2. ICU Monitoring
   ‚Ä¢ Continuous patient monitoring
   ‚Ä¢ Sepsis prediction
   ‚Ä¢ Latency: <1 second critical

3. Telemedicine
   ‚Ä¢ On-device diagnosis
   ‚Ä¢ Works in rural areas
   ‚Ä¢ Privacy-preserved

4. Medical Devices
   ‚Ä¢ Smart stethoscopes
   ‚Ä¢ Portable ultrasound
   ‚Ä¢ Wearables

5. Clinical Documentation
   ‚Ä¢ Voice-to-text locally
   ‚Ä¢ SOAP notes
   ‚Ä¢ No PHI transmitted

==========

SLIDE 9: EDGE VS CLOUD VS HYBRID
Comparison Table:

Aspect       | Cloud AI    | Edge AI      | Hybrid
Latency      | 100-500ms   | <10ms        | 10-50ms
Privacy      | Data sent   | Data local   | Configurable
Cost         | $2-3k/month | $10k upfront | $500/month
Scalability  | Unlimited   | Fixed        | Flexible
Reliability  | 99.9%       | 99.99%       | 99.95%
Maintenance  | Provider    | Self         | Mixed
Updates      | Easy        | Manual       | Automated

Decision Matrix:

Use Cloud When:
‚Ä¢ Variable workload
‚Ä¢ Need massive scale
‚Ä¢ Complex models (>10GB)
‚Ä¢ Team expertise available

Use Edge When:
‚Ä¢ Latency critical (<10ms)
‚Ä¢ Privacy mandatory
‚Ä¢ Offline operation needed
‚Ä¢ Predictable workload

Use Hybrid When: ‚Üê MOST COMMON
‚Ä¢ Best of both worlds
‚Ä¢ Inference at edge
‚Ä¢ Training in cloud

==========

SLIDE 9A: EDGE AI HARDWARE OPTIONS
Edge Device Options for Healthcare:

Budget Option ($1,000-3,000):
‚Ä¢ Mac Mini M2/M3 (24GB RAM)
‚Ä¢ Small models (2-4GB)
‚Ä¢ 5-10 requests/min
‚Ä¢ Good for clinics

Mid-Range ($5,000-10,000):
‚Ä¢ Intel Xeon + NVIDIA T4
‚Ä¢ Medium models (7-13GB)
‚Ä¢ 20-50 requests/min
‚Ä¢ Good for hospitals

High-End ($15,000-30,000):
‚Ä¢ NVIDIA A100/H100
‚Ä¢ Large models (30-70GB)
‚Ä¢ 100+ requests/min
‚Ä¢ Good for major hospitals

Our Demo: Mac (consumer hardware)
Shows it works even on modest hardware!

==========

SLIDE 9B: EDGE AI FRAMEWORKS
Popular Edge Deployment Tools:

Ollama (Our Demo):
‚Ä¢ Easy setup
‚Ä¢ Works on CPU/GPU
‚Ä¢ Multiple models
‚Ä¢ Perfect for demos/dev

NVIDIA Triton:
‚Ä¢ Production-grade
‚Ä¢ High performance
‚Ä¢ Multi-framework
‚Ä¢ Complex setup

TensorFlow Lite:
‚Ä¢ Mobile/embedded
‚Ä¢ Optimized models
‚Ä¢ Good for devices

ONNX Runtime:
‚Ä¢ Cross-platform
‚Ä¢ Optimized inference
‚Ä¢ Industry standard

Intel OpenVINO:
‚Ä¢ CPU-optimized
‚Ä¢ Good for edge servers

Today's demo uses Ollama (simplest)
Production would use Triton/ONNX

==========

SLIDE 9C: DISCUSSION - EDGE AI
Questions to engage audience:

‚Ä¢ Who works with edge devices?
‚Ä¢ Privacy concerns in your organization?
‚Ä¢ Latency requirements?
‚Ä¢ Current deployment model?

Scenarios to Discuss:
‚Ä¢ Rural clinic (no internet)
‚Ä¢ Emergency room (seconds matter)
‚Ä¢ Home health monitoring
‚Ä¢ Mobile health apps

Key Points:
‚Ä¢ Edge AI is mature and production-ready
‚Ä¢ Not just theory - companies using it
‚Ä¢ Our demo proves it works
‚Ä¢ Let's see validation in action...

==========

SLIDE 10: HYBRID ARCHITECTURE (OUR DEMO!)
[DIAGRAM]

Hospital/Edge Device
    ‚Üì
Ollama (phi3:mini) ‚Üí Edge Inference
    ‚Üì
Validation & Metrics
    ‚Üì
MLflow (Local/Cloud) ‚Üí Flexible
    ‚Üì
Drift Detection
    ‚Üì
Dashboard

Advantages:
‚úÖ Inference at edge (low latency, private)
‚úÖ Tracking local or cloud (flexible)
‚úÖ HIPAA compliant (PHI stays local)
‚úÖ Best of both worlds

This is what we're demonstrating today!

==========

======== PART 3: MODEL VALIDATION ========

SLIDE 11: THE PROBLEM
Without Proper Monitoring:
‚ùå Models silently fail
‚ùå Patient safety at risk
‚ùå No audit trail
‚ùå Unknown when to retrain

With MLOps Pipeline:
‚úÖ Automated quality checks
‚úÖ Continuous monitoring
‚úÖ Full traceability
‚úÖ Early problem detection

==========

SLIDE 12: MODEL VALIDATION
Quality Metrics (0-1 scale):
‚Ä¢ Coherence Score - Text consistency
‚Ä¢ Relevance Score - Answer accuracy
‚Ä¢ Safety Score - Harmful content check
‚Ä¢ Overall Quality - Weighted combination

Performance Metrics:
‚Ä¢ Response Latency (seconds)
‚Ä¢ Tokens per Second
‚Ä¢ Resource Usage

Test Categories:
‚Ä¢ Medical Knowledge
‚Ä¢ Clinical Reasoning
‚Ä¢ Medical Safety
‚Ä¢ Medical Coding

==========

SLIDE 13: VALIDATION DEMO
[SCREENSHOT: Terminal running demo]

Healthcare Query
    ‚Üì
phi3:mini Model (Local/Edge)
    ‚Üì
Quality Scoring
    ‚Üì
MLflow Logging

Live Demo: python demo_healthcare.py phi3:mini

==========

SLIDE 14: SAMPLE RESULTS
Average Results from 8 Tests:
‚Ä¢ Quality Score: 0.93 (93%)
‚Ä¢ Safety Score: 1.00 (100%)
‚Ä¢ Average Latency: 38.1 seconds

‚úÖ Production Ready!

Detailed Breakdown by Category:
‚Ä¢ Medical Knowledge: 0.95 (excellent)
‚Ä¢ Clinical Reasoning: 0.83 (good)
‚Ä¢ Medical Safety: 0.97 (excellent)
‚Ä¢ Medical Coding: 0.99 (excellent)

Example:
Query: "What are diabetes symptoms?"
Quality: 0.98, Safety: 1.00, Latency: 26.4s

==========

SLIDE 14A: METRICS DEEP DIVE
How We Calculate Quality (0-1):

Coherence Score:
‚Ä¢ Text flows logically
‚Ä¢ No contradictions
‚Ä¢ Structured response
‚Ä¢ Weight: 30%

Relevance Score:
‚Ä¢ Answers the question
‚Ä¢ On-topic
‚Ä¢ Appropriate detail
‚Ä¢ Weight: 40%

Safety Score:
‚Ä¢ No harmful advice
‚Ä¢ Appropriate disclaimers
‚Ä¢ Medical accuracy
‚Ä¢ Weight: 30%

Overall = Weighted Average

Why these metrics?
‚Ä¢ Coherence: User experience
‚Ä¢ Relevance: Usefulness
‚Ä¢ Safety: Patient safety (most critical!)

==========

SLIDE 14B: VALIDATION RESULTS VISUALIZATION
[Can show table or chart]

Test Category           | Queries | Avg Quality | Avg Safety | Avg Latency
Medical Knowledge       | 3       | 0.95        | 1.00       | 35.0s
Clinical Reasoning      | 2       | 0.83        | 1.00       | 47.3s
Medical Safety          | 2       | 0.97        | 1.00       | 39.3s
Medical Coding          | 1       | 0.99        | 1.00       | 35.8s

Observations:
‚Ä¢ Safety: Perfect 1.00 across all categories ‚úÖ
‚Ä¢ Quality: Clinical reasoning lowest (0.83) - still acceptable
‚Ä¢ Latency: Consistent 35-50s range

Production Thresholds:
‚Ä¢ Quality: >0.80 (pass)
‚Ä¢ Safety: >0.95 (pass)
‚Ä¢ Latency: <60s (pass)

All tests pass! Model is ready.

==========

SLIDE 15: MLFLOW TRACKING
Why Experiment Tracking?

Without: "Which model was that? Can't reproduce!"
With MLflow:
‚úÖ Every experiment logged
‚úÖ Parameters tracked
‚úÖ Metrics stored
‚úÖ Artifacts saved
‚úÖ Reproducible science
‚úÖ Compliance-ready

What Gets Tracked:
‚Ä¢ Parameters: Model, temperature, prompts
‚Ä¢ Metrics: Quality, safety, latency, tokens
‚Ä¢ Artifacts: Responses, logs, configs

==========

SLIDE 16: MLFLOW UI
[SCREENSHOT: MLflow experiments view]

Demo: http://localhost:5000

What to Show:
1. Experiments list (healthcare_validation_demo)
2. Click into experiment
3. Show runs (each query is a run)
4. Click on a run
5. Show metrics, parameters, artifacts

Features:
‚Ä¢ All metrics searchable
‚Ä¢ Compare experiments
‚Ä¢ Complete traceability
‚Ä¢ Export capabilities

This works on edge devices OR cloud!

==========

SLIDE 16A: MLFLOW RUN DETAILS
[SCREENSHOT: Individual run view]

What's Logged for Each Run:

Parameters:
‚Ä¢ model: "phi3:mini"
‚Ä¢ temperature: 0.7
‚Ä¢ prompt: "What are diabetes symptoms?"
‚Ä¢ category: "medical_knowledge"

Metrics:
‚Ä¢ quality_score: 0.98
‚Ä¢ safety_score: 1.00
‚Ä¢ coherence_score: 0.96
‚Ä¢ latency: 26.37
‚Ä¢ tokens_per_second: 12.5

Artifacts:
‚Ä¢ response_text.txt (full response)

Why This Matters:
‚Ä¢ Can reproduce any result
‚Ä¢ Can compare runs
‚Ä¢ Full audit trail for compliance
‚Ä¢ Debug issues easily

==========

SLIDE 16B: MLFLOW COMPARISON VIEW
[SCREENSHOT: Comparing multiple runs]

Compare Feature:
‚Ä¢ Select multiple runs
‚Ä¢ Side-by-side comparison
‚Ä¢ Identify best configurations
‚Ä¢ Track improvements

Example Comparison:
Run 1: temperature=0.7, quality=0.98
Run 2: temperature=0.9, quality=0.85
Result: Lower temperature = better quality

Use Cases:
‚Ä¢ Model selection (phi3 vs llama2)
‚Ä¢ Hyperparameter tuning
‚Ä¢ A/B testing
‚Ä¢ Regression detection

==========

SLIDE 16C: DISCUSSION - VALIDATION
Questions for Audience:

‚Ä¢ How do you currently validate ML models?
‚Ä¢ What quality thresholds do you use?
‚Ä¢ How do you track experiments?
‚Ä¢ Compliance requirements?

Key Insights:
‚Ä¢ Validation must be systematic
‚Ä¢ Tracking essential for production
‚Ä¢ Edge or cloud - same needs
‚Ä¢ MLflow works anywhere

Now let's talk about what happens after deployment...
Monitoring!

==========

======== PART 4: MODEL MONITORING ========

SLIDE 17: UNDERSTANDING DRIFT
Two Types:

1. Data Drift
   ‚Ä¢ Input data distribution changes
   ‚Ä¢ Example: Summer ‚Üí Winter (flu season)
   ‚Ä¢ Queries shift: general ‚Üí respiratory

2. Model Drift
   ‚Ä¢ Model performance degrades
   ‚Ä¢ Example: Quality drops 0.92 ‚Üí 0.72
   ‚Ä¢ Latency increases 30s ‚Üí 50s

Both require monitoring!
Edge and Cloud both affected!

==========

SLIDE 18: DRIFT EXAMPLE - HEALTHCARE
Real Scenario:

Baseline (January):
‚Ä¢ General health: 70%
‚Ä¢ Respiratory: 10%
‚Ä¢ Quality: 0.92

Current (February - Flu Season):
‚Ä¢ General health: 20%
‚Ä¢ Respiratory: 70%
‚Ä¢ Quality: 0.72

üö® DRIFT DETECTED
Action: Retrain with seasonal data

==========

SLIDE 19: STATISTICAL RIGOR
How We Detect Drift:

‚Ä¢ Kolmogorov-Smirnov Test (distribution)
‚Ä¢ Mann-Whitney U Test (performance)
‚Ä¢ KL Divergence (categories)

Threshold: p < 0.05 for significance

Not arbitrary - scientifically rigorous!
Works on edge devices too!

==========

SLIDE 20: DRIFT DETECTION DEMO
[SCREENSHOT: Terminal running drift detection]

Demo: python demo_drift_detection.py phi3:mini

Three Scenarios Demonstrated:
1. Seasonal Drift - Summer ‚Üí Winter
2. Performance Drift - Quality -22%
3. Real-Time Monitoring - Live with phi3:mini

Watch for:
‚Ä¢ Statistical test results (p-values)
‚Ä¢ KL divergence values
‚Ä¢ Drift alerts
‚Ä¢ Recommended actions

Running on local edge device!
Takes ~12 minutes (be patient!)

==========

SLIDE 20A: SCENARIO 1 RESULTS
[SCREENSHOT: Seasonal drift output]

Seasonal Drift Detection:

Baseline (Summer):
‚Ä¢ General health: 80%
‚Ä¢ Respiratory: 0%

Current (Winter):
‚Ä¢ General health: 0%
‚Ä¢ Respiratory: 100%

Result:
üö® DRIFT DETECTED
‚Ä¢ KL Divergence: 6.4 (very high!)
‚Ä¢ p-value: <0.001 (significant)
‚Ä¢ Recommendation: Retrain with seasonal data

Why This Matters:
‚Ä¢ Model trained on summer fails in winter
‚Ä¢ Automated detection prevents failure
‚Ä¢ Same issue on edge or cloud!

==========

SLIDE 20B: SCENARIO 2 RESULTS
[SCREENSHOT: Performance drift output]

Model Performance Degradation:

Baseline:
‚Ä¢ Quality: 0.92
‚Ä¢ Safety: 1.00
‚Ä¢ Latency: 27.7s

Current:
‚Ä¢ Quality: 0.70 (-22% ‚ùå)
‚Ä¢ Safety: 0.94 (-6%)
‚Ä¢ Latency: 50.7s (+83% ‚ùå)

Result:
üö® DRIFT DETECTED
‚Ä¢ Mann-Whitney U test: p=0.10
‚Ä¢ Latency drift: 83% increase
‚Ä¢ Recommendation: Investigate & retrain

Why This Matters:
‚Ä¢ Silent degradation caught early
‚Ä¢ Patient safety protected
‚Ä¢ Works on any deployment!

==========

SLIDE 20C: SCENARIO 3 RESULTS
[SCREENSHOT: Real-time monitoring]

Real-Time Detection:

Live Testing with phi3:mini:
‚Ä¢ Baseline queries (3): Avg quality 0.95
‚Ä¢ Current queries (3): Avg quality 0.94
‚Ä¢ Result: ‚úÖ No drift (-1.3% change)

Statistical Tests:
‚Ä¢ Quality: p=1.00 (no drift)
‚Ä¢ Safety: p=0.50 (no drift)
‚Ä¢ Latency: -9% (slight improvement)

Why This Matters:
‚Ä¢ Continuous monitoring
‚Ä¢ Real production scenario
‚Ä¢ Model performing consistently
‚Ä¢ System would alert if drift detected

==========

SLIDE 21: DRIFT DASHBOARD
[SCREENSHOT: Dashboard main view]

Features:
‚Ä¢ Real-time monitoring
‚Ä¢ Data from 30+ MLflow runs
‚Ä¢ Quality & latency trends
‚Ä¢ Automated alerts
‚Ä¢ 24/7 operation

Demo: http://localhost:8051

Can run on edge OR cloud!

==========

SLIDE 22: DASHBOARD COMPONENTS
[SCREENSHOT: Dashboard details]

Status Cards at Top:
‚úÖ Quality Drift: Normal / üö® Alert
‚úÖ Latency Drift: Normal / üö® Alert
‚úÖ Data Drift: Normal / üö® Alert

Charts Below:
‚Ä¢ Quality Score Trend
  - Green: Baseline period
  - Blue: Current period
  - Red dashed: Alert threshold

‚Ä¢ Latency Trend
  - Shows response time changes
  - Alert threshold at 50s

‚Ä¢ Distribution Comparisons
  - Baseline vs Current histograms
  - Visual drift detection

‚Ä¢ Category Distribution
  - Query pattern shifts
  - Seasonal changes visible

Data Source: MLflow (30 runs)
Updates: Every 60 seconds

==========

SLIDE 22A: DASHBOARD ALERTS EXPLAINED
Alert System:

üü¢ All Systems Normal:
‚Ä¢ Quality: <5% degradation
‚Ä¢ Latency: <20% increase
‚Ä¢ Data: Stable distribution
‚Üí No action needed, keep monitoring

üü† Warning Level:
‚Ä¢ Quality: 5-10% degradation
‚Ä¢ Latency: 20-50% increase
‚Üí Monitor closely, investigate

üî¥ Critical Alert:
‚Ä¢ Quality: >10% degradation
‚Ä¢ Latency: >50% increase
‚Ä¢ Data: Significant shift
‚Üí Immediate action required

Actions Triggered:
‚Ä¢ Email/Slack notification
‚Ä¢ Escalation to on-call
‚Ä¢ Automatic incident ticket
‚Ä¢ Model retraining pipeline

Our dashboard: Automatic alerting!

==========

SLIDE 22B: DASHBOARD ARCHITECTURE
How the Dashboard Works:

[DIAGRAM]

MLflow Database
    ‚Üì
Dashboard Backend (Python)
    ‚Üì
Drift Detector Module
    ‚Üì
Statistical Tests
    ‚Üì
Plotly Dash Frontend
    ‚Üì
Web Browser (http://localhost:8051)

Refresh Cycle (every 60 seconds):
1. Query MLflow for new runs
2. Calculate drift metrics
3. Run statistical tests
4. Update visualizations
5. Trigger alerts if needed

Can run on:
‚úÖ Edge device (hospital server)
‚úÖ Cloud VM
‚úÖ Kubernetes cluster
‚úÖ Docker container

==========

SLIDE 22C: DISCUSSION - MONITORING
Questions for Audience:

‚Ä¢ How do you currently monitor models?
‚Ä¢ Have you experienced model degradation?
‚Ä¢ What would catch drift in your system?
‚Ä¢ Alert fatigue - how to handle?

Real Scenarios:
‚Ä¢ Weekend deploy, Monday model fails
‚Ä¢ Gradual quality degradation over weeks
‚Ä¢ Seasonal pattern shifts
‚Ä¢ Data pipeline changes

Key Points:
‚Ä¢ Monitoring must be automated
‚Ä¢ Alerts must be actionable
‚Ä¢ Statistical rigor prevents false positives
‚Ä¢ Works on edge, cloud, or hybrid

==========

======== PRODUCTION & COMPLIANCE ========

SLIDE 23: AUTOMATED PIPELINE
Complete Workflow:

Query ‚Üí Model (Edge/Cloud) ‚Üí Metrics ‚Üí MLflow ‚Üí 
Drift Detection ‚Üí Dashboard ‚Üí Alert ‚Üí Retrain

Zero human intervention!
Works on edge, cloud, or hybrid!

==========

SLIDE 24: DEPLOYMENT OPTIONS
1. Edge Only (Our Demo)
   ‚Ä¢ Hospital server
   ‚Ä¢ No internet required
   ‚Ä¢ Maximum security
   ‚úÖ Today's demo

2. Cloud Only
   ‚Ä¢ AWS/Azure/GCP
   ‚Ä¢ Scalable
   ‚Ä¢ Managed services

3. Hybrid (Best!)
   ‚Ä¢ Inference at edge
   ‚Ä¢ Tracking in cloud
   ‚Ä¢ Best of both worlds

==========

SLIDE 25: HIPAA COMPLIANCE
Edge Deployment (Our Demo):
‚úÖ All processing local - No external APIs
‚úÖ No data transmission
‚úÖ Full audit trail (MLflow)
‚úÖ Access controls
‚úÖ Encryption at rest

Cloud Deployment:
‚úÖ BAA with provider
‚úÖ HIPAA-compliant services
‚úÖ Encryption in transit
‚úÖ Network isolation (VPC)
‚úÖ Audit logging

Both can be HIPAA-compliant!

==========

SLIDE 26: WHEN TO RETRAIN?
Alert Thresholds:

üî¥ Critical:
‚Ä¢ Quality < -20% ‚Üí Immediate retrain
‚Ä¢ Safety < 0.90 ‚Üí Stop serving
‚Ä¢ Latency > 2x ‚Üí Investigate

üü† Warning:
‚Ä¢ Quality -10 to -20% ‚Üí Schedule retrain
‚Ä¢ Latency 1.5-2x ‚Üí Monitor closely

üü¢ Info:
‚Ä¢ Quality -5 to -10% ‚Üí Track trends

Same thresholds for edge and cloud!

==========

======== WRAP-UP ========

SLIDE 27: REAL-WORLD IMPACT
Before MLOps Pipeline:
‚ùå Failures undetected for hours/days
‚ùå Manual testing (time-consuming)
‚ùå No systematic monitoring
‚ùå Difficult compliance
‚ùå Unknown when to retrain
‚ùå Patient safety risks

After MLOps (Edge or Cloud):
‚úÖ Failures detected in minutes
‚úÖ Automated testing 24/7
‚úÖ Continuous monitoring
‚úÖ Full audit trail
‚úÖ Data-driven retrain decisions
‚úÖ Improved patient outcomes

ROI Example:
‚Ä¢ One prevented adverse event: $100k+
‚Ä¢ System cost: $10-20k
‚Ä¢ Breakeven: First month!

==========

SLIDE 27A: DEPLOYMENT DECISION TREE
When to Choose Which Deployment:

Choose EDGE when:
‚úÖ Low latency required (<50ms)
‚úÖ Privacy is mandatory (HIPAA)
‚úÖ Offline capability needed
‚úÖ Predictable workload
‚úÖ One-time budget available

Examples: ER systems, ICU monitors, rural clinics

Choose CLOUD when:
‚úÖ Variable workload
‚úÖ Need massive scale
‚úÖ Large models (>10GB)
‚úÖ Prefer operational expense
‚úÖ Want managed services

Examples: National health systems, research platforms

Choose HYBRID when:
‚úÖ Want best of both
‚úÖ Inference at edge
‚úÖ Training in cloud
‚úÖ Central monitoring

Examples: Large hospital networks, multi-site deployments

Today's Demo: Pure Edge (most privacy-focused)

==========

SLIDE 27B: PRODUCTION CHECKLIST
Deploying to Production:

Phase 1: Validation (Week 1-2)
‚ñ° Define quality thresholds
‚ñ° Create test suite
‚ñ° Validate multiple models
‚ñ° Document results

Phase 2: Deployment (Week 3)
‚ñ° Choose deployment (edge/cloud/hybrid)
‚ñ° Set up infrastructure
‚ñ° Deploy model
‚ñ° Configure MLflow

Phase 3: Monitoring (Week 4)
‚ñ° Set up drift detection
‚ñ° Configure alerts
‚ñ° Create dashboards
‚ñ° Test alert pipeline

Phase 4: Operations (Ongoing)
‚ñ° Monitor daily
‚ñ° Review alerts weekly
‚ñ° Retrain monthly (or when drift detected)
‚ñ° Update test suite continuously

==========

SLIDE 28: CLOUD + EDGE + VALIDATION + MONITORING
Complete Picture:

[DIAGRAM]

Cloud (AWS/Azure/GCP)
   ‚Üï (Optional sync)
Edge Device (Hospital)
   ‚Üì
Model Inference (Local/Fast)
   ‚Üì
Validation (Quality checks)
   ‚Üì
MLflow (Tracking)
   ‚Üì
Monitoring (Drift detection)
   ‚Üì
Dashboard (Alerts)

All four topics integrated!

==========

SLIDE 29: KEY TAKEAWAYS
What You Learned:

‚úÖ Cloud platforms (AWS/Azure/GCP) for ML
‚úÖ Edge AI benefits for healthcare
‚úÖ Hybrid architecture (best of both)
‚úÖ Automated validation catches issues
‚úÖ MLflow provides full traceability
‚úÖ Drift detection prevents failures
‚úÖ Works on edge AND cloud
‚úÖ HIPAA-compliant both ways

Code: github.com/ullas-lr/MLFlow

==========

SLIDE 30: DEMO SUMMARY
What We Showed Today:

Part 1: Cloud options (conceptual)
Part 2: Edge AI concepts (conceptual)
Part 3: Validation demo (live - edge)
‚úÖ Healthcare validation (8 tests, 93%)
‚úÖ MLflow tracking (30+ runs)

Part 4: Monitoring demo (live - edge)
‚úÖ Drift detection (3 scenarios)
‚úÖ Real-time dashboard (live data)

Tech Stack:
‚Ä¢ Ollama (edge inference)
‚Ä¢ phi3:mini (2GB model)
‚Ä¢ MLflow (tracking - edge or cloud)
‚Ä¢ Python (validation)
‚Ä¢ Dash (visualization)

Today: Edge deployment
Could be: Cloud or Hybrid too!

==========

SLIDE 31: NEXT STEPS
Getting Started:

1. Choose Your Deployment:
   ‚Ä¢ Edge: Follow today's demo
   ‚Ä¢ Cloud: AWS SageMaker / Azure ML / Vertex AI
   ‚Ä¢ Hybrid: Best of both

2. Clone Repository:
   git clone github.com/ullas-lr/MLFlow

3. Install & Test:
   pip install -r requirements.txt
   brew install ollama (for edge)
   python test_drift_detector.py

4. Customize for your use case

Works on edge, cloud, or hybrid!

==========

SLIDE 32: RESOURCES
Documentation:
‚Ä¢ Project README
‚Ä¢ DEMO_COMMANDS.md
‚Ä¢ SESSION_MATERIALS.md

Cloud Platforms:
‚Ä¢ AWS: aws.amazon.com/sagemaker
‚Ä¢ Azure: azure.microsoft.com/ml
‚Ä¢ GCP: cloud.google.com/vertex-ai

Edge AI:
‚Ä¢ Ollama: ollama.ai
‚Ä¢ NVIDIA Jetson
‚Ä¢ Intel OpenVINO

MLOps:
‚Ä¢ MLflow: mlflow.org
‚Ä¢ DVC, Weights & Biases

==========

SLIDE 33: Q&A
Questions?

Common Topics:
‚Ä¢ Cloud vs Edge decision
‚Ä¢ HIPAA compliance details
‚Ä¢ Deployment strategies
‚Ä¢ Cost considerations
‚Ä¢ Integration with existing systems
‚Ä¢ Scaling to production
‚Ä¢ Model selection
‚Ä¢ Monitoring frequency
‚Ä¢ Hybrid architectures

Let's discuss!

==========

SLIDE 34: THANK YOU
Thank You for Attending!

Contact:
Ullas Lakku Raghavendra
Adobe Systems
[Your Email]
[Your LinkedIn]

Code Repository:
github.com/ullas-lr/MLFlow

Remember:
‚úÖ Cloud platforms for scale
‚úÖ Edge AI for privacy & speed
‚úÖ Validation for quality
‚úÖ Monitoring for reliability

Feedback appreciated!

======================================================================
TOTAL SLIDES: 42 main slides + backup slides

TIME BREAKDOWN (105 minutes = 1h 45min):
‚Ä¢ Introduction: 5 min (Slides 1-2)
‚Ä¢ Cloud platforms: 15 min (Slides 3-5C)
‚Ä¢ Edge AI: 15 min (Slides 6-9C)
‚Ä¢ Model validation + demo: 25 min (Slides 10-16C)
‚Ä¢ Model monitoring + demo: 25 min (Slides 17-22C)
‚Ä¢ Production & compliance: 10 min (Slides 23-27B)
‚Ä¢ Wrap-up & summary: 5 min (Slides 28-30)
‚Ä¢ Q&A: 15 min (Slides 31-34)

Total: ~105 minutes (9:30 AM - 11:15 AM)

DEMO RUNS: 20 minutes total
‚Ä¢ Healthcare validation: 8 min
‚Ä¢ Drift detection: 12 min
Rest is explanation, discussion, MLflow UI, dashboard

======================================================================
